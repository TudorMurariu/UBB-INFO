{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "1. softmax\n",
    "\n",
    "\n",
    "```py\n",
    "f(x) = x * W\n",
    "\n",
    "self.W = (input_shape, )\n",
    "w.requires_grad()\n",
    "s = x @ self.W\n",
    "\n",
    "s = torch.functional.softmax(s, dim=1)\n",
    "\n",
    "torch.argmax(s, dim=1)\n",
    "\n",
    "# save and load\n",
    " \n",
    "```\n",
    "### fit:\n",
    "* epochs\n",
    "* batch\n",
    "  \n",
    "```py\n",
    "for epoch in range(epochs):\n",
    "    for i in x_train.shape[0] // bs:\n",
    "        x_s = x_train[i * bs, (i+1) * bs]\n",
    "        y_b = y_train\n",
    "        s = x_s @ self.W\n",
    "        s = log_softmax(s)\n",
    "        loss = ce(s, y-s)           + reg.strength() # if we use regularization\n",
    "        loss.b()\n",
    "\n",
    "self.W == self.W.grad * h \n",
    "W.zerp_grad()\n",
    "\n",
    "---------------\n",
    "# Regularization\n",
    "np.square(self.W).sum()\n",
    "```\n",
    "\n",
    "Softmax: https://ogunlao.github.io/2020/04/26/you_dont_really_know_softmax.html"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
